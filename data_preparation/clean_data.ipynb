{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning May 5th 2022 data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../datasets/old/tripdata_2022_05_05.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused columns\n",
    "columns_to_drop = [\n",
    "    'fecha', 'idBike', 'fleet', 'locktype', 'unlocktype',\n",
    "    'address_lock', 'address_unlock', 'dock_unlock', 'dock_lock'\n",
    "]\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates\n",
    "def extract_coordinates(geo_str):\n",
    "    try:\n",
    "        geo_dict = ast.literal_eval(geo_str)\n",
    "        lon, lat = geo_dict['coordinates']\n",
    "        return pd.Series([lat, lon])\n",
    "    except:\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "df[['unlock_lat', 'unlock_lon']] = df['geolocation_unlock'].apply(extract_coordinates)\n",
    "df[['lock_lat', 'lock_lon']] = df['geolocation_lock'].apply(extract_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all_stations.csv\n",
    "station_df = df[['station_unlock', 'unlock_station_name', 'unlock_lat', 'unlock_lon']].drop_duplicates()\n",
    "station_df = station_df.rename(columns={\n",
    "    'station_unlock': 'station_id',\n",
    "    'unlock_station_name': 'station_name',\n",
    "    'unlock_lat': 'lat',\n",
    "    'unlock_lon': 'lon'\n",
    "})\n",
    "\n",
    "# Drop rows with missing values\n",
    "station_df = station_df.dropna()\n",
    "\n",
    "# Sort by station_id\n",
    "station_df = station_df.sort_values(by='station_id')\n",
    "\n",
    "station_df.to_csv('../datasets/all_stations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all_trips.csv\n",
    "trip_df = df[[\n",
    "    'id', 'unlock_date', 'lock_date',\n",
    "    'station_unlock', 'station_lock',\n",
    "    'trip_minutes', 'unlock_lat', 'unlock_lon',\n",
    "    'lock_lat', 'lock_lon'\n",
    "]]\n",
    "\n",
    "trip_df = trip_df.rename(columns={\n",
    "    'id': 'trip_id',\n",
    "    'unlock_date': 'start_time',\n",
    "    'lock_date': 'end_time',\n",
    "    'station_unlock': 'start_station_id',\n",
    "    'station_lock': 'end_station_id'\n",
    "})\n",
    "\n",
    "# Convert time columns to datetime\n",
    "trip_df['start_time'] = pd.to_datetime(trip_df['start_time'])\n",
    "trip_df['end_time'] = pd.to_datetime(trip_df['end_time'])\n",
    "\n",
    "# Remove rows with missing values\n",
    "trip_df = trip_df.dropna()\n",
    "\n",
    "# Haversine distance function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Convert degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "# Calculate distance for each trip\n",
    "trip_df['distance_km'] = trip_df.apply(\n",
    "    lambda row: haversine(row['unlock_lat'], row['unlock_lon'], row['lock_lat'], row['lock_lon']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save trips\n",
    "trip_df.to_csv('../datasets/all_trips_05_05.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning May 11th 2022 data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../datasets/old/tripdata_2022_05_11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused columns\n",
    "columns_to_drop = [\n",
    "    'fecha', 'idBike', 'fleet', 'locktype', 'unlocktype',\n",
    "    'address_lock', 'address_unlock', 'dock_unlock', 'dock_lock'\n",
    "]\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates\n",
    "def extract_coordinates(geo_str):\n",
    "    try:\n",
    "        geo_dict = ast.literal_eval(geo_str)\n",
    "        lon, lat = geo_dict['coordinates']\n",
    "        return pd.Series([lat, lon])\n",
    "    except:\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "df[['unlock_lat', 'unlock_lon']] = df['geolocation_unlock'].apply(extract_coordinates)\n",
    "df[['lock_lat', 'lock_lon']] = df['geolocation_lock'].apply(extract_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all_trips_05_11.csv\n",
    "trip2_df = df[[\n",
    "    'id', 'unlock_date', 'lock_date',\n",
    "    'station_unlock', 'station_lock',\n",
    "    'trip_minutes', 'unlock_lat', 'unlock_lon',\n",
    "    'lock_lat', 'lock_lon'\n",
    "]]\n",
    "\n",
    "trip2_df = trip2_df.rename(columns={\n",
    "    'id': 'trip_id',\n",
    "    'unlock_date': 'start_time',\n",
    "    'lock_date': 'end_time',\n",
    "    'station_unlock': 'start_station_id',\n",
    "    'station_lock': 'end_station_id'\n",
    "})\n",
    "\n",
    "# Convert time columns to datetime\n",
    "trip2_df['start_time'] = pd.to_datetime(trip2_df['start_time'])\n",
    "trip2_df['end_time'] = pd.to_datetime(trip2_df['end_time'])\n",
    "\n",
    "# Remove rows with missing values\n",
    "trip2_df = trip2_df.dropna()\n",
    "\n",
    "# Haversine distance function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Convert degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "# Calculate distance for each trip\n",
    "trip2_df['distance_km'] = trip2_df.apply(\n",
    "    lambda row: haversine(row['unlock_lat'], row['unlock_lon'], row['lock_lat'], row['lock_lon']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save trips\n",
    "trip2_df.to_csv('../datasets/all_trips_05_11.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all data that is not between 00:00:00 and 23:59:59 for both days\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files with datetime parsing\n",
    "df_05_05 = pd.read_csv(\"../datasets/all_trips_05_05.csv\", parse_dates=[\"start_time\", \"end_time\"])\n",
    "df_05_11 = pd.read_csv(\"../datasets/all_trips_05_11.csv\", parse_dates=[\"start_time\", \"end_time\"])\n",
    "\n",
    "# Define the valid cutoff times\n",
    "cutoff_05_05 = pd.Timestamp(\"2022-05-05 23:59:59\")\n",
    "cutoff_05_11 = pd.Timestamp(\"2022-05-11 23:59:59\")\n",
    "\n",
    "# Filter out trips ending after midnight\n",
    "df_05_05 = df_05_05[df_05_05[\"end_time\"] <= cutoff_05_05]\n",
    "df_05_11 = df_05_11[df_05_11[\"end_time\"] <= cutoff_05_11]\n",
    "\n",
    "# Overwrite the original files with cleaned data\n",
    "df_05_05.to_csv(\"../datasets/all_trips_05_05.csv\", index=False)\n",
    "df_05_11.to_csv(\"../datasets/all_trips_05_11.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Converting the old stations ids (int) to the ones corresponding them from the station name (str)\n",
    "import pandas as pd\n",
    "\n",
    "stations_df = pd.read_csv(\"../datasets/all_stations.csv\")\n",
    "\n",
    "# This grabs everything before the first space or \" -\"\n",
    "stations_df['new_station_id'] = stations_df['station_name'].str.extract(r'^(\\S+)')\n",
    "\n",
    "station_id_mapping = stations_df[['station_id', 'new_station_id']].copy()\n",
    "station_id_mapping.columns = ['old_station_id', 'new_station_id']\n",
    "station_id_mapping = station_id_mapping.dropna()\n",
    "\n",
    "# Convert old ID to float (in case itâ€™s stored as \"69.0\" in trips)\n",
    "station_id_mapping['old_station_id'] = station_id_mapping['old_station_id'].astype(float)\n",
    "\n",
    "# Ensure new ID is string (it will be anyway, but explicitly)\n",
    "station_id_mapping['new_station_id'] = station_id_mapping['new_station_id'].astype(str)\n",
    "\n",
    "station_id_mapping.to_csv(\"../datasets/old/station_id_mapping.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the new stations ids to the old ones in all files\n",
    "import pandas as pd\n",
    "\n",
    "# Load mapping\n",
    "mapping_df = pd.read_csv(\"../datasets/old/station_id_mapping.csv\")\n",
    "id_map = mapping_df.set_index('old_station_id')['new_station_id'].to_dict()\n",
    "\n",
    "stations_df = pd.read_csv(\"../datasets/all_stations.csv\")\n",
    "trips_05_05 = pd.read_csv(\"../datasets/all_trips_05_05.csv\")\n",
    "trips_05_11 = pd.read_csv(\"../datasets/all_trips_05_11.csv\")\n",
    "\n",
    "# Update station_id in stations file\n",
    "stations_df['station_id'] = stations_df['station_id'].astype(float).map(id_map)\n",
    "\n",
    "# Update start and end station IDs in both trip files\n",
    "for df in [trips_05_05, trips_05_11]:\n",
    "    df['start_station_id'] = df['start_station_id'].astype(float).map(id_map)\n",
    "    df['end_station_id'] = df['end_station_id'].astype(float).map(id_map)\n",
    "\n",
    "    # Drop rows that failed to map\n",
    "    df.dropna(subset=['start_station_id', 'end_station_id'], inplace=True)\n",
    "\n",
    "stations_df.to_csv(\"../datasets/all_stations.csv\", index=False)\n",
    "trips_05_05.to_csv(\"../datasets/all_trips_05_05.csv\", index=False)\n",
    "trips_05_11.to_csv(\"../datasets/all_trips_05_11.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bike Trip Env",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
